---
title: "QuantifyingLearning"
author: "Jonas Mikhaeil"
date: "2025-06-11"
output: html_document
---
```{r}
library(ggplot2)
library(transport)
```

## Showcasing the Wasserstein-2 learning metric

Based on (Green et al.,2016), suppose we begin with a $\pi_0 =$normal(0,5) prior on the effect of lawn signs on candidate's vote share (the scale being percent). A subsequent study finds an effect of 2.5 (1.7) and the prior is updated to a $\pi_1 =$normal(2.2,1.6). In this example, our learning metric yields 
$W_2(\pi_0,\pi_1) = \sqrt{2.2^2+(5-1.6)^2}=4.0$. 
Negative effects may apriori be unreasonable. To take this into account, a more complicated prior could be used. We modify the original prior $\pi_0$ by moving all negative mass to $0$.
The following code shows how our learning metric can be used to quantify learning even in the case of non-normal distributions:

```{r}
### sample from the original normal prior
samples <- rnorm(100000,0,5)

### create the modified prior that excludes negative effeects
modified_samples <- ifelse(samples < 0, 0, samples)
modified_samples <- data.frame(samples=modified_samples)
ggplot(modified_samples, aes(x = samples)) +
  geom_density(color = "black", size = 1) +
  labs(title = "Density Plot (Line Only)",
       x = "Value",
       y = "Density") +
  theme_minimal()
### use importance sampling to calculate the posterior
likelihoods <- dnorm(2.5, mean = modified_samples$samples, sd = 1.7)  
w_unnorm   <- likelihoods
w <- w_unnorm / sum(w_unnorm)
posterior_draws <- sample(modified_samples$samples, size = 100000, replace = TRUE, prob = w)
posterior_draws <- data.frame(samples = posterior_draws)

ggplot(data.frame(posterior_draws), aes(x = samples)) +
  geom_density(color = "dodgerblue4", size = 1) +
  labs(title = "Density Plot (Line Only)",
       x = "Parameter",
       y = "Density") +
  theme_minimal()+
 geom_density(data=modified_samples,aes(x=samples),color = "black", size = 1) +
  labs(title = "Density Plot (Line Only)",
       x = "Parameter",
       y = "Density") +
  theme_minimal()
### calculate the Wasserstein-2 distance between prior and posterior
w2=transport::wasserstein1d(posterior_draws$samples,modified_samples$samples,p=2)
w2
```


## Code to recreate central figures
### Figure 1

The following code creates the densities for Figure 1:
```{r}
library(ggplot2)

# Create a sequence of x values
x <- seq(-20, 20, length.out = 1000)

# Define the parameters
means <- c( 0, 10)
sds <- c( 1, 3)
colors <- c( "#FFDAB9","pink2")  # Pastel blue, peach, green, pink

# Create a data frame for plotting
densities <- data.frame()

for (i in 1:2) {
  y <- dnorm(x, mean = means[i], sd = sds[i])
  temp <- data.frame(x = x, y = y, group = paste0("Dist ", i))
  densities <- rbind(densities, temp)
}

# Plot
#pdf("Figure1.pdf",5,4)
ggplot(densities, aes(x = x, y = y, fill = group, color = group)) +
  geom_area(alpha = 0.3, position = 'identity') +  # Shaded areas
  scale_fill_manual(values = colors) +             # Use pastel colors
  scale_color_manual(values = colors) +             # Outline in same colors
  theme_minimal() +
  labs(x = latex2exp::TeX("Parameter $\\theta$"),
       y = "Density",
       fill = "Distribution") +
  theme(
    text = element_text(size = 14),
    legend.position = "none"
  )+theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))+
  xlim(-5,20)

```

### Prospective Learning
The following code creates the figures for the prospective learning section:

#### Figure 4
```{r}
n <- 1000
w <- 1/2
com_prior <- rnorm(n,3,1)
pioneer_prior <- rnorm(n,0,3)
draws <- rbinom(n,1,w)
dc_prior <- draws*com_prior +( 1-draws) * pioneer_prior
  

y_c = com_prior + rnorm(n,0,0.1)
y_p = pioneer_prior + rnorm(n,0,0.1)
y_dc = dc_prior + rnorm(n,0,0.1)

df <- data.frame(
  value = c(unlist(y_c), unlist(y_p),unlist(y_dc)),
  group = factor(rep(c("consensus", "pioneer","decision-maker"), each = n))
)
df
colors <- c( "#FFDAB9","#BFD8B8","pink2")  # Pastel blue, peach, green, pink
#pdf("prosp_prior.pdf",5,4)
ggplot(df, aes(x = value, fill = group, color = group)) +
geom_density(alpha = 0.5, position = "identity", bins = 30) +
  scale_fill_manual(values = colors) +             # Use pastel colors
  scale_color_manual(values = colors) +             # Outline in same colors
  theme_minimal() +
  labs(x = latex2exp::TeX("Prior $\\theta$"),
       y = "Density",
       fill = "Distribution") +
  theme(
    text = element_text(size = 14),
    legend.position = "none"
  )+theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))
```
```{r}
y_dc = 2.5 + rnorm(n,0,0.1) #2.5
y_dc <- data.frame(y_dc=y_dc,color="blue")
y_dc
#pdf("prosp_pred_2.pdf",5,4)
ggplot(y_dc, aes(x = y_dc,fill=color,color=color)) +
geom_density(alpha = 0.5, position = "identity") +
  scale_fill_manual(values = "steelblue") +             # Use pastel colors
  scale_color_manual(values = "steelblue") +             # Outline in same colors
  theme_minimal() +
  labs(x = latex2exp::TeX("Data $y$"),
       y = "Density",
       fill = "Distribution") +
  theme(
    text = element_text(size = 14),
    legend.position = "none"
  )+theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))+
   scale_x_continuous(limits = c(-1.5,4), expand = c(-0.04, 0))
```
```{r}
y_dc = -0.5 + rnorm(1,0,0.1)
likelihoods <- dnorm(y_dc, mean = dc_prior, sd = 1)  
w_unnorm   <- likelihoods
w <- w_unnorm / sum(w_unnorm)
posterior_draws <- sample(dc_prior, size = n, replace = TRUE, prob = w)
posterior_draws <- data.frame(pd = posterior_draws,color="blue")

#pdf("prosp_posterior_1.pdf",5,4)
ggplot(posterior_draws, aes(x = pd,fill=color,color=color)) +
geom_density(alpha = 0.5, position = "identity") +
  scale_fill_manual(values = "#BFD8B8") +             # Use pastel colors
  scale_color_manual(values = "#BFD8B8") +             # Outline in same colors
  theme_minimal() +
  labs(x = latex2exp::TeX("Posterior $\\theta|y$"),
       y = "Density",
       fill = "Distribution") +
  theme(
    text = element_text(size = 14),
    legend.position = "none"
  )+theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))+
   scale_x_continuous(limits = c(-8,8), expand = c(-0.04, 0))
```

```{r}


w <- 1/2
n <- 5000
n_study <- 100
com_prior <- rnorm(n,3,1)
pioneer_prior <- rnorm(n,0,3)
draws <- rbinom(n,1,w)
dc_prior <- draws*com_prior +( 1-draws) * pioneer_prior
y_c = com_prior + rnorm(n,0,10/sqrt(n_study))
y_p = pioneer_prior + rnorm(n,0,10/sqrt(n_study))
y_dc = dc_prior + rnorm(n,0,10/sqrt(n_study))
W2_l <- NULL
for(y in y_dc){
likelihoods <- dnorm(y, mean = dc_prior, sd = 10/sqrt(n_study))  
w_unnorm   <- likelihoods
w <- w_unnorm / sum(w_unnorm)
posterior_draws <- sample(dc_prior, size = n, replace = TRUE, prob = w)
w2=transport::wasserstein1d(posterior_draws,com_prior,p=2)
W2_l <- rbind(W2_l,w2=w2)
W2_l <- data.frame(W2_l)
}
W2_mean <- mean(W2_l$W2_l)


```

```{r}
W2_l$color = "blue"
#pdf("prosp_pred_learning.pdf")
ggplot(W2_l, aes(x = W2_l,fill=color,color=color)) +
geom_density(alpha = 0.5, position = "identity") +
  scale_fill_manual(values = "grey") +             # Use pastel colors
  scale_color_manual(values = "grey") +             # Outline in same colors
  theme_minimal() +
  labs(x = latex2exp::TeX("Predictive Wasserstein-2 Distribution "),
       y = "Density",
       fill = "Distribution") +
  theme(
    text = element_text(size = 14),
    legend.position = "none"
  )+theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))+
   scale_x_continuous(limits = c(0,10), expand = c(-0.04, 0))
```


#### Figure 5

```{r}
library(transport)
# if you want parallel:
# install.packages("future.apply"); library(future.apply)
# future::plan(future::multisession, workers = parallel::detectCores()-1)

study_sizes <- c(100,1000)
weights     <- c(0, 0.2, 1/3, 1/2, 2/3, 0.8, 1)
n           <- 5000

# set up the output data frame
results <- expand.grid(
  n_study = study_sizes,
  w       = weights,
  KEEP.OUT.ATTRS = FALSE,
  stringsAsFactors = FALSE
)
results$w2_mean <- NA_real_

for(i in seq_len(nrow(results))) {
  n_study <- results$n_study[i]
  w       <- results$w[i]
  sd_y    <- 10 / sqrt(n_study)
  
  # draw priors once
  com_prior     <- rnorm(n, 3, 1)
  pioneer_prior <- rnorm(n, 0, 3)
  draws         <- rbinom(n, 1, w)
  dc_prior      <- draws * com_prior + (1 - draws) * pioneer_prior
  
  # simulate all y_dc at once
  y_dc <- dc_prior + rnorm(n, 0, sd_y)
  
  # pre-allocate vector to hold W2 for each y
  W2_vec <- numeric(n)
  
  # (optional) sort com_prior once to speed up wasserstein
  com_sorted <- sort(com_prior)
  
  # inner loop
  for(j in seq_len(n)) {
    # importance weights ∝ likelihood
    w_unnorm <- dnorm(y_dc[j], mean = dc_prior, sd = sd_y)
    w_norm   <- w_unnorm / sum(w_unnorm)
    
    # sample posterior and compute W2
    post_draws    <- sample(dc_prior, size = n, replace = TRUE, prob = w_norm)
    W2_vec[j]     <- transport::wasserstein1d(post_draws, com_sorted, p = 2)
  }
  
  results$w2_mean[i] <- mean(W2_vec)
}

```




```{r}
plot1<-ggplot(results, aes(x=1-w,y=w2_mean,color=as.factor(n_study),group =as.factor(n_study)))+
    geom_line(linewidth=1)+
  geom_point()+
  scale_color_manual(values = c( "dodgerblue4","dodgerblue1")) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),legend.position = "none")+
  xlab("Decision-maker weights")+
   ylab("Prospective Learning")+
   annotate(geom="text", x=0.8, y=2, label=latex2exp::TeX("n = 100"),color="dodgerblue4")+
   annotate(geom="text", x=0.3, y=2.5, label=latex2exp::TeX("n = 1000"),color="dodgerblue1")
#pdf("prospective_learning_weights.pdf",5,4)
plot1
```




